{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70e3a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCitation: https://www.kaggle.com/code/divyansh22/lgbm-classifier-for-airline-recommendation/notebook\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\"\"\"\n",
    "Citation: https://www.kaggle.com/code/divyansh22/lgbm-classifier-for-airline-recommendation/notebook\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aefd11",
   "metadata": {},
   "source": [
    "## Step 1: load the dataset and prepare the data for training##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84826e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "CSV_PATH = \"../2/dmt-2025-2nd-assignment/training_set_VU_DM.csv\"\n",
    "reader = pd.read_csv(CSV_PATH,nrows=250_000_0)\n",
    "df = reader.copy()\n",
    "\n",
    "CSV_PATH2 = \"../2/dmt-2025-2nd-assignment/test_set_VU_DM.csv\"\n",
    "reader2 = pd.read_csv(CSV_PATH2)\n",
    "df2 = reader2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db28d78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 801. MiB for an array with shape (42, 2500000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# define user features\u001b[39;00m\n\u001b[0;32m     16\u001b[0m user_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitor_location_country_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrch_destination_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n\u001b[1;32m---> 20\u001b[0m df_groupable \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprop_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:6686\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid how option: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask):\n\u001b[1;32m-> 6686\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6688\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc(axis\u001b[38;5;241m=\u001b[39maxis)[mask]\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6811\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6662\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   6664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6665\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6666\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6809\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   6810\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6811\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6814\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6815\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2294\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2287\u001b[0m new_values: ArrayLike\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   2290\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[1;32m-> 2294\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2296\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tangx\\miniconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 801. MiB for an array with shape (42, 2500000) and data type float64"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "df['price_vs_mean'] = df['price_usd'] - df.groupby('srch_id')['price_usd'].transform('mean')\n",
    "df['price_rank'] = df.groupby('srch_id')['price_usd'].rank()\n",
    "df['review_rank'] = df.groupby('srch_id')['prop_review_score'].rank()\n",
    "df[\"people count\"] = df[\"srch_adults_count\"] + df[\"srch_children_count\"]\n",
    "\n",
    "df2['price_rank'] = df2.groupby('srch_id')['price_usd'].rank()\n",
    "df2['review_rank'] = df2.groupby('srch_id')['prop_review_score'].rank()\n",
    "df2[\"people count\"] = df2[\"srch_adults_count\"] + df[\"srch_children_count\"]\n",
    "\n",
    "# interaction features\n",
    "df['price_review_interaction'] = df['price_usd'] * df['prop_review_score']\n",
    "df2['price_review_interaction'] = df2['price_usd'] * df2['prop_review_score']\n",
    "\n",
    "# define user features\n",
    "user_features = [\n",
    "    \"visitor_location_country_id\",\n",
    "    \"srch_destination_id\",\n",
    "]\n",
    "df_groupable = df.dropna(subset=user_features + ['prop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27d81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "df_groupable[\"srch_destination_id\"] = pd.qcut(\n",
    "    df['visitor_hist_adr_usd'].fillna(df['visitor_hist_adr_usd'].median()),\n",
    "    q=4,\n",
    "    duplicates='drop'\n",
    ")\n",
    "df_groupable['visitor_location_country_id'] = pd.qcut(\n",
    "    df['visitor_hist_adr_usd'].fillna(df['visitor_hist_adr_usd'].median()),\n",
    "    q=10,\n",
    "    duplicates='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee866ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tangx\\AppData\\Local\\Temp\\ipykernel_24956\\3710086840.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg = df_groupable.groupby(user_features + ['prop_id']).agg(\n"
     ]
    }
   ],
   "source": [
    "# aggregate user features to get click and booking rates\n",
    "agg = df_groupable.groupby(user_features + ['prop_id']).agg(\n",
    "    sim_user_click_rate=('click_bool', 'mean'),\n",
    "    sim_user_book_rate=('booking_bool', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# merge back to the original dataframe\n",
    "df = df.merge(agg, on=user_features + ['prop_id'], how='left') # train\n",
    "df2 = df2.merge(agg, on=user_features + ['prop_id'], how='left') # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "460de953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing values\n",
    "df['sim_user_click_rate'] = df['sim_user_click_rate'].fillna(0)\n",
    "df['sim_user_book_rate'] = df['sim_user_book_rate'].fillna(0)\n",
    "df2['sim_user_click_rate'] = df2['sim_user_click_rate'].fillna(0)\n",
    "df2['sim_user_book_rate'] = df2['sim_user_book_rate'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14efc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that assign weights to the labels\n",
    "def assign_weight(df, booking_label, click_label, booking_weight, click_weight, default_weight):\n",
    "    \"\"\"\n",
    "    This function assigns weights to the labels based on the number of clicks and bookings\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['label'] = df['booking_bool'] * booking_label + df['click_bool'] * click_label\n",
    "    \n",
    "    # assign weight according to the label\n",
    "    df['weight'] = default_weight\n",
    "    df.loc[(df['click_bool'] == 1) & (df['booking_bool'] == 0), 'weight'] = click_weight\n",
    "    df.loc[df['booking_bool'] == 1, 'weight'] = booking_weight\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5ff9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     2388229\n",
      "15      69679\n",
      "5       42092\n",
      "Name: count, dtype: int64\n",
      "Train set size: (2374566, 63)\n",
      "Validation set size: (125434, 63)\n"
     ]
    }
   ],
   "source": [
    "# combined labels for click an booking\n",
    "booking_label = 10\n",
    "click_label = 5\n",
    "booking_weight = 10\n",
    "click_weight = 5\n",
    "default_weight = 1\n",
    "df_labeled = assign_weight(df, booking_label, click_label, booking_weight, click_weight, default_weight)\n",
    "\n",
    "# group search session by user id\n",
    "unique_searches = df_labeled['srch_id'].unique()\n",
    "\n",
    "# split the train and test set\n",
    "search_train, search_va = train_test_split(unique_searches, test_size=0.05, random_state=42)\n",
    "\n",
    "# create a train and test dataset\n",
    "train_df = df_labeled[df_labeled['srch_id'].isin(search_train)].sort_values('srch_id')\n",
    "val_df  = df_labeled[df_labeled['srch_id'].isin(search_va)].sort_values('srch_id')\n",
    "print(df_labeled['label'].value_counts())\n",
    "print(f\"Train set size: {train_df.shape}\")\n",
    "print(f\"Validation set size: {val_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe57dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group train size: 2374566\n",
      "X_train size: (2374566, 18)\n"
     ]
    }
   ],
   "source": [
    "# prepare the input and labels for the model\n",
    "features = [\n",
    "        \"srch_length_of_stay\",\n",
    "        \"srch_booking_window\",\n",
    "        \"people count\",\n",
    "        \"srch_room_count\",\n",
    "        \"srch_saturday_night_bool\",\n",
    "        \"prop_review_score\",\n",
    "        \"prop_starrating\",\n",
    "        \"price_usd\",\n",
    "        \"promotion_flag\",\n",
    "        \"prop_brand_bool\",\n",
    "        \"prop_location_score1\",\n",
    "        \"prop_log_historical_price\",\n",
    "        \"price_vs_mean\",      # relative\n",
    "        \"star_diff\",          # relative\n",
    "        \"review_diff\",         # relative\n",
    "        'price_review_interaction',\n",
    "        \"sim_user_click_rate\",\n",
    "        \"sim_user_book_rate\",\n",
    "        ]\n",
    "X_train = train_df[features]\n",
    "X_val = val_df[features]\n",
    "y_train = train_df['label']\n",
    "y_val = val_df['label'] \n",
    "group_train = train_df.groupby('srch_id').size().to_list()\n",
    "\n",
    "# these two parts should have the same size\n",
    "print(f\"Group train size: {sum(group_train)}\")\n",
    "print(f\"X_train size: {X_train.shape}\")\n",
    "\n",
    "# create a validation set\n",
    "val_group = val_df.groupby('srch_id').size().to_list()\n",
    "val_set = lgb.Dataset(X_val, label=y_val, group=val_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f1048",
   "metadata": {},
   "source": [
    "## Step 2: train the LGBM model with LGBM training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c1f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Calculating query weights...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2086\n",
      "[LightGBM] [Info] Number of data points in the train set: 2374566, number of used features: 16\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "train_data = lgb.Dataset(X_train, label=y_train, weight = train_df['weight'], group=group_train)\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_set],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a70d1",
   "metadata": {},
   "source": [
    "## Step 3: Make and prediction and produce output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33dc8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    srch_id  prop_id      score  rank\n",
      "23        1    99484  78.893356   1.0\n",
      "12        1    61934  78.869016   2.0\n",
      "20        1    90385  78.867132   3.0\n",
      "14        1    72090  78.863138   4.0\n",
      "9         1    54937  78.859816   5.0\n",
      "4         1    24194  78.858536   6.0\n",
      "6         1    34263  78.852761   7.0\n",
      "7         1    37567  78.848721   8.0\n",
      "5         1    28181  78.835857   9.0\n",
      "22        1    95031  78.831040  10.0\n",
      "8         1    50162  78.824122  11.0\n",
      "18        1    82231  78.817031  12.0\n",
      "0         1     3180  78.807989  13.0\n",
      "17        1    78599  78.804626  14.0\n",
      "1         1     5543  78.796179  15.0\n",
      "16        1    74045  78.793331  16.0\n",
      "25        1   128085  78.776872  17.0\n",
      "24        1   123675  78.773399  18.0\n",
      "3         1    22393  78.750752  19.0\n",
      "2         1    14142  78.750126  20.0\n"
     ]
    }
   ],
   "source": [
    "# prediction on the test set\n",
    "X_test = df2[features]\n",
    "df2['score'] = model.predict(X_test)\n",
    "df2['rank'] = df2.groupby('srch_id')['score'].rank(ascending=False)\n",
    "output = df2[['srch_id', 'prop_id', 'score', 'rank']].sort_values(['srch_id', 'rank'])\n",
    "print(output.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56be99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 0.0967\n",
      "NDCG@5: 0.2896\n"
     ]
    }
   ],
   "source": [
    "# evaluate the ranking \n",
    "val_df['score'] = model.predict(X_val)\n",
    "val_df['rank'] = val_df.groupby('srch_id')['score'].rank(ascending=False, method='first')\n",
    "\n",
    "# Step 6: Evaluate Hit@1 and NDCG@5\n",
    "top_preds = val_df[val_df['rank'] == 1]\n",
    "hit_rate = (top_preds['booking_bool'] == 1).mean()\n",
    "\n",
    "ndcg_list = []\n",
    "for srch_id, group in val_df.groupby('srch_id'):\n",
    "    y_true = group['label'].values.reshape(1, -1)\n",
    "    y_score = group['score'].values.reshape(1, -1)\n",
    "    ndcg = ndcg_score(y_true, y_score, k=5)\n",
    "    ndcg_list.append(ndcg)\n",
    "average_ndcg = np.mean(ndcg_list)\n",
    "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
    "print(f\"NDCG@5: {average_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5262c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the output as a csv file\n",
    "submission = output[['srch_id', 'prop_id']].copy()\n",
    "submission.to_csv('hotel_rankings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0c2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
