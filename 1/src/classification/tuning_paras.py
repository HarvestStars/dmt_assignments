from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# === Step 1: load dataset ===
train_df = pd.read_csv("./source_data/train_split_by_id.csv")
test_df = pd.read_csv("./source_data/test_split_by_id.csv")

# === Step 2: define the features cols ===
features = [col for col in train_df.columns if col.endswith('_hist_mean')]
target = 'mood_type'

X_train = train_df[features]
y_train = train_df[target]
X_test = test_df[features]
y_test = test_df[target]
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5],
    'class_weight': [None, 'balanced']
}

clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')
clf.fit(X_train, y_train)

print("æœ€ä½³å‚æ•°ç»„åˆï¼š", clf.best_params_)


# === Step 4: è¿›è¡Œé¢„æµ‹ ===
y_pred = clf.predict(X_test)

# === Step 5: è¯„ä¼°æŒ‡æ ‡ ===
acc = accuracy_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
report = classification_report(y_test, y_pred)

# === Step 6: æ‰“å°ç»“æœ ===
print("âœ… Accuracy:", acc)
print("ğŸ“‰ MAE:", mae)
print("ğŸ“‰ MSE:", mse)
print("\nğŸ“‹ Classification Report:\n", report)

# === Step 7: å¯è§†åŒ–æ··æ·†çŸ©é˜µ ===
# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Low (<7)", "High (â‰¥7)"])
disp.plot(cmap="Blues", values_format='d')

# è¯„ä¼°æŒ‡æ ‡
print("Accuracy:", accuracy_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=["Low (<7)", "High (â‰¥7)"]))

# ç»Ÿè®¡å„ç±»åˆ«æ•°é‡ï¼ˆçœŸå®å’Œé¢„æµ‹ï¼‰
actual_counts = pd.Series(y_test).value_counts().sort_index()
pred_counts = pd.Series(y_pred).value_counts().sort_index()

# ç¡®ä¿ä¸¤ä¸ª Series æœ‰ç›¸åŒç´¢å¼•ï¼ˆ0 å’Œ 1ï¼‰
all_classes = [0, 1]
actual_counts = actual_counts.reindex(all_classes, fill_value=0)
pred_counts = pred_counts.reindex(all_classes, fill_value=0)

# åˆå¹¶åˆ°ä¸€ä¸ª DataFrame
compare_df = pd.DataFrame({
    'Actual': actual_counts,
    'Predicted': pred_counts
})

# ç»˜å›¾
compare_df.plot(kind='bar', figsize=(7, 5), color=['orange', 'steelblue'])
plt.title("Actual vs Predicted Class Counts (Test Set)")
plt.xlabel("Mood Type (Binary)")
plt.ylabel("Count")
plt.xticks(ticks=[0, 1], labels=["Low (<7)", "High (â‰¥7)"], rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()